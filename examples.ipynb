{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain and LangSmith Notes\n",
    "\n",
    "## Import the Supporting Modules for this Notebook\n",
    "\n",
    "These commands were ran with Python 3.11.8.\n",
    "\n",
    "To load the dependencies for this project using `pip`, follow these steps:\n",
    "\n",
    "1. Open a terminal or command prompt.\n",
    "2. Navigate to the directory where your project is located.\n",
    "3. Create a virtual environment (optional but recommended):\n",
    "   - Run `python3 -m venv env` to create a virtual environment named \"env\".\n",
    "   - Activate the virtual environment:\n",
    "     - On Windows, run `env\\Scripts\\activate`.\n",
    "     - On macOS and Linux, run `source env/bin/activate`.\n",
    "   - I use pyenv and the pyenv virtualenv wrapper for this so it is up to you on how you would like to do this. \n",
    "4. Install the dependencies:\n",
    "   - Run `pip install -r requirements.txt` to install the dependencies listed in the \"requirements.txt\" file.\n",
    "5. Once the installation is complete, you can use the dependencies in your project.\n",
    "\n",
    "To load the dependencies for this project using `poetry`, follow these steps:\n",
    "\n",
    "1. Open a terminal or command prompt.\n",
    "2. Navigate to the directory where your project is located.\n",
    "3. If you haven't already, install `poetry` by running `pip install poetry`.\n",
    "4. Create a virtual environment (optional but recommended):\n",
    "   - Run `poetry env use python3` to create a virtual environment using Python 3.\n",
    "   - Activate the virtual environment by running `poetry shell`.\n",
    "5. Install the dependencies:\n",
    "   - Run `poetry install` to install the dependencies specified in the \"pyproject.toml\" file.\n",
    "   - Poetry will create a virtual environment and install the dependencies within it.\n",
    "6. Once the installation is complete, you can use the dependencies in your project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from random import choice\n",
    "from uuid import uuid4\n",
    "from datetime import datetime\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display\n",
    "from requests import get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import LangChain Modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_mistralai import ChatMistralAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain Globals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_debug, set_verbose\n",
    "from langchain.callbacks.tracers import ConsoleCallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Keys\n",
    "\n",
    "Be sure you create the appropiate .env file in your folder. This includes your Langsmith Keys.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "mistral_api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "openweather_api_key = os.getenv(\"OPENWEATHER_API_KEY\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Template Creation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"You are a very friendly and helpful Assistant. Please help the user with their queries. User: {query}\"\n",
    "prompt = ChatPromptTemplate.from_template(template=template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pirate Template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pirate_template = \"Avast, me hearty! Ye be a friendly and helpful Assistant. Translate this sentence into Pirate, the language of the high seas. I be askin' ye to translate this here passage fer me, if ye please. {passage}\"\n",
    "pirate_prompt = ChatPromptTemplate.from_template(template=pirate_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM creation\n",
    "\n",
    "In this example we are using Google Gemini and Anthropics Claude 2.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_llm = ChatGoogleGenerativeAI(google_api_key=google_api_key, model=\"gemini-pro\")\n",
    "anthropic_llm = ChatAnthropic(api_key=anthropic_api_key, model=\"claude-3-opus-20240229\")\n",
    "mistral_llm = ChatMistralAI(model=\"mistral-large-latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LCEL Creation\n",
    "\n",
    "### Basic Query\n",
    "\n",
    "Here we'll create a few basic invokes and chains for our demonstration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic query\n",
    "\n",
    "basic_query_g = prompt | gemini_llm | StrOutputParser()\n",
    "basic_query_a = prompt | anthropic_llm | StrOutputParser()\n",
    "basic_query_m = prompt | mistral_llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chaining\n",
    "\n",
    "Next let's create a simple chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple chain that translates an LLM output into pirate\n",
    "\n",
    "# User's query -> Google LLM -> Google LLM output -> Anthropic LLM -> Anthropic LLM output in Pirate\n",
    "anthropic_translates = pirate_prompt | anthropic_llm\n",
    "pirate_query = {\"passage\": basic_query_g} | anthropic_translates | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API chain\n",
    "\n",
    "To make it a little more interesting we can also try an API chain. First let's create a simple function that makes an api call to Openweathermap.org\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple API call to get the current weather\n",
    "\n",
    "\n",
    "def get_weather(weather_api):\n",
    "    api_data = weather_api.get(\"weather_api\")\n",
    "    weather_api_response = get(\n",
    "        f\"https://api.openweathermap.org/data/3.0/onecall?lat={api_data.lat}&lon={api_data.lon}&units={api_data.units}&appid={openweather_api_key}\"\n",
    "    )\n",
    "    weather_api_response.raise_for_status()\n",
    "    dt = datetime.fromtimestamp(weather_api_response.json()[\"current\"][\"dt\"])\n",
    "    formatted_time = (\n",
    "        f'{dt.strftime(\"%I:%M %p\")} Timezone={weather_api_response.json()[\"timezone\"]}'\n",
    "    )\n",
    "    return {\n",
    "        \"local time\": formatted_time,\n",
    "        \"curent weather\": weather_api_response.json()[\"current\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above you will notice I'm using the weather_api object in the function above. I will use Pydantic with Langchain. This allows helps ensure that the corret data is given and in a format that is usesable for my function. If I wanted to go further I could have added validation tests to ensure the data is valid for the api.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the Schema\n",
    "\n",
    "Here I'd like to take a brief detour here from LCEL and LangSmith and show something that is important as we start working with structured output. LangChain helps us implement a Pydantic Model for our structured output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create the model for our output parser\n",
    "from typing import Literal\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the docstrings here are crucial, as they will be passed along to the model along with the class name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherLookup(BaseModel):\n",
    "    \"\"\"Multiply two integers together.\"\"\"\n",
    "\n",
    "    lat: float = Field(\n",
    "        ...,\n",
    "        title=\"Latitude\",\n",
    "        description=\"This is the latitude of the location you want to look up the weather for\",\n",
    "    )\n",
    "    lon: float = Field(\n",
    "        ...,\n",
    "        title=\"Longitude\",\n",
    "        description=\"This is the longitude of the location you want to look up the weather for\",\n",
    "    )\n",
    "    units: Literal[\"imperial\", \"metric\"] = Field(\n",
    "        \"imperial\",\n",
    "        title=\"Unit\",\n",
    "        description=\"This is the unit you want the temperature to be in (imperial for F or metric for C)\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a prompt for the user to make a query in natural language to ask for the weather in any desired location. The docsstrings in the Pydantic Model tell the LLM how the output should be structured. While it is possible to just provide the query without instruction, by providing instructions, the LLM can provide more accurate results and more focused responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the template for the LLM to understand our request.\n",
    "weather_template = \"You are a very friendly and helpful Assistant. Please help the user with their weather related queries. User: {query}\"\n",
    "weather_prompt = ChatPromptTemplate.from_template(template=weather_template)\n",
    "\n",
    "# We will create a weather man prompt for the generative ouptut at the end of the chain\n",
    "\n",
    "weather_man_template = \"This is the response from api.openweather.com about the current weather conditions. Based on this response, can you please create a friendly weatherman report and include relevant emojis that are fun and personalized to me? Also, please provide the current time, which is provided in the API output you have received as well.\\n\\n{weather_api_response}\"\n",
    "weather_man_prompt = ChatPromptTemplate.from_template(template=weather_man_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure the LLM and the Chain\n",
    "\n",
    "There are couple things going on here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use Mistral for the first step, for the structure of the output\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "\n",
    "structured_llm_m = mistral_llm.with_structured_output(WeatherLookup)\n",
    "\n",
    "retrieve_location_chain = {\n",
    "    \"weather_api\": (weather_prompt | structured_llm_m)\n",
    "} | RunnableLambda(get_weather)\n",
    "\n",
    "weather_output = weather_man_prompt | gemini_llm | StrOutputParser()\n",
    "\n",
    "api_chain = {\"weather_api_response\": retrieve_location_chain} | weather_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangSmith\n",
    "\n",
    "First let's run the chains and then review them in LangSmith.\n",
    "\n",
    "To use the out of the box functionality you just need to have the following in your environment:\n",
    "\n",
    "```ini\n",
    "# Langchain Settings\n",
    "LANGCHAIN_TRACING_V2=true\n",
    "LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
    "LANGCHAIN_API_KEY=<API_KEY>\n",
    "LANGCHAIN_PROJECT=barry-local-dev # this is the project name the traces will be found in LangSmith\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the basic query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_query_g.invoke(\n",
    "    {\n",
    "        \"query\": \"Can you create a social media post for my blog about Artificial Intelligence and its impacts on our daily lives?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_query_a.invoke(\n",
    "    {\n",
    "        \"query\": \"Can you create a social media post for my blog about Artificial Intelligence and its impacts on our daily lives?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_query_m.invoke(\n",
    "    {\n",
    "        \"query\": \"Can you create a social media post for my blog about Artificial Intelligence and its impacts on our daily lives?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the simple chain\n",
    "\n",
    "Let's run a simple chain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So that we can see the conversation in realtime let's turn on verbose mode\n",
    "set_verbose(True)\n",
    "\n",
    "print(\n",
    "    pirate_query.invoke(\n",
    "        {\n",
    "            \"query\": \"In regard to George Lucas's Space opera, Empire Strikes Back, can you recite the dialogue between Darth Vadar and Luke Skywalker during their duel on Bespin when Darth Vadar reveals to Luke that he is Luke's Father? Please provide the dialoge only.\"\n",
    "        },\n",
    "        config={\"callbacks\": [ConsoleCallbackHandler()]},\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the API Chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    api_chain.invoke(\n",
    "        {\n",
    "            \"query\": \"What is the weather like at Disney World today in Orlando, Florida?\"\n",
    "        },\n",
    "        config={\"callbacks\": [ConsoleCallbackHandler()]},\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring Tagging and Metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagging\n",
    "\n",
    "Let's turn off verbosity for now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_verbose(False)  # Turn off verbose mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One whay of adding tags and metadata to a chain is the `with_config()` method of the `Runnable` class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tagging the chains\n",
    "\n",
    "# We are using the keyword \"run_name\" to tag the run\n",
    "\n",
    "basic_query_g = basic_query_g.with_config({\"run_name\": \"Basic Query with Google LLM\"})\n",
    "basic_query_a = basic_query_a.with_config(\n",
    "    {\"run_name\": \"Basic Query with Anthropic LLM\"}\n",
    ")\n",
    "basic_query_m = basic_query_m.with_config({\"run_name\": \"Basic Query with Mistral LLM\"})\n",
    "\n",
    "google_output = basic_query_g.invoke(\n",
    "    {\n",
    "        \"query\": \"Can you create a social media post for my blog about Artificial Intelligence and its impacts on our daily lives?\"\n",
    "    }\n",
    ")\n",
    "anthropic_output = basic_query_a.invoke(\n",
    "    {\n",
    "        \"query\": \"Can you create a social media post for my blog about Artificial Intelligence and its impacts on our daily lives?\"\n",
    "    }\n",
    ")\n",
    "mistral_output = basic_query_m.invoke(\n",
    "    {\n",
    "        \"query\": \"Can you create a social media post for my blog about Artificial Intelligence and its impacts on our daily lives?\"\n",
    "    }\n",
    ")\n",
    "display(\n",
    "    Markdown(\n",
    "        f\"\"\"\n",
    "                 **Google Output**\n",
    "                 \n",
    "                 {google_output}\n",
    "                 \n",
    "                 **Anthropic Output**\n",
    "                 \n",
    "                 {anthropic_output}\n",
    "                 \n",
    "                 **Mistral Output**\n",
    "                 \n",
    "                 {mistral_output}\n",
    "                 \"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's tag the remaining chains\n",
    "priate_query = pirate_query.with_config({\"run_name\": \"Pirate Translate Chain\"})\n",
    "api_chain = api_chain.with_config({\"run_name\": \"Weather API Chain\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "priate_query.invoke(\n",
    "    {\n",
    "        \"query\": \"In regard to George Lucas's Space opera, Empire Strikes Back, can you recite the dialogue between Darth Vadar and Luke Skywalker during their duel on Bespin when Darth Vadar reveals to Luke that he is Luke's Father? Please provide the dialoge only.\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_chain.invoke({\"query\": \"Whats the weather like on Chicago in Farenheit?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata\n",
    "\n",
    "First the class RunnableConfig will need to be imported.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if our application were tracking the user and the session id we can add that as meta data to the conversation. RunnableConfig also can be used to update the the following:\n",
    "\n",
    "```python\n",
    "\"\"\"Configuration for a Runnable.\"\"\"\n",
    "\n",
    "tags: List[str]\n",
    "\"\"\"\n",
    "Tags for this call and any sub-calls (eg. a Chain calling an LLM).\n",
    "You can use these to filter calls.\n",
    "\"\"\"\n",
    "\n",
    "metadata: Dict[str, Any]\n",
    "\"\"\"\n",
    "Metadata for this call and any sub-calls (eg. a Chain calling an LLM).\n",
    "Keys should be strings, values should be JSON-serializable.\n",
    "\"\"\"\n",
    "\n",
    "callbacks: Callbacks\n",
    "\"\"\"\n",
    "Callbacks for this call and any sub-calls (eg. a Chain calling an LLM).\n",
    "Tags are passed to all callbacks, metadata is passed to handle*Start callbacks.\n",
    "\"\"\"\n",
    "\n",
    "run_name: str\n",
    "\"\"\"\n",
    "Name for the tracer run for this call. Defaults to the name of the class.\n",
    "\"\"\"\n",
    "\n",
    "max_concurrency: Optional[int]\n",
    "\"\"\"\n",
    "Maximum number of parallel calls to make. If not provided, defaults to\n",
    "ThreadPoolExecutor's default.\n",
    "\"\"\"\n",
    "\n",
    "recursion_limit: int\n",
    "\"\"\"\n",
    "Maximum number of times a call can recurse. If not provided, defaults to 25.\n",
    "\"\"\"\n",
    "\n",
    "configurable: Dict[str, Any]\n",
    "\"\"\"\n",
    "Runtime values for attributes previously made configurable on this Runnable,\n",
    "or sub-Runnables, through .configurable_fields() or .configurable_alternatives().\n",
    "Check .output_schema() for a description of the attributes that have been made\n",
    "configurable.\n",
    "\"\"\"\n",
    "\n",
    "run_id: Optional[uuid.UUID]\n",
    "\"\"\"\n",
    "Unique identifier for the tracer run for this call. If not provided, a new UUID\n",
    "    will be generated.\n",
    "\"\"\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = choice([\"barweiss@cisco.com\", \"user1@cisco.com\", \"user2@cisco.com\"])\n",
    "session_id = str(uuid4())\n",
    "google_output = basic_query_g.invoke(\n",
    "    {\n",
    "        \"query\": \"Can you create a social media post for my blog about Artificial Intelligence and its impacts on our daily lives?\"\n",
    "    },\n",
    "    RunnableConfig(metadata={\"username\": username, \"session_id\": session_id}),\n",
    ")\n",
    "\n",
    "username = choice([\"barweiss@cisco.com\", \"user1@cisco.com\", \"user2@cisco.com\"])\n",
    "session_id = str(uuid4())\n",
    "anthropic_output = basic_query_a.invoke(\n",
    "    {\n",
    "        \"query\": \"Can you create a social media post for my blog about Artificial Intelligence and its impacts on our daily lives?\"\n",
    "    },\n",
    "    RunnableConfig(metadata={\"username\": username, \"session_id\": session_id}),\n",
    ")\n",
    "\n",
    "username = choice([\"barweiss@cisco.com\", \"user1@cisco.com\", \"user2@cisco.com\"])\n",
    "session_id = str(uuid4())\n",
    "mistral_output = basic_query_m.invoke(\n",
    "    {\n",
    "        \"query\": \"Can you create a social media post for my blog about Artificial Intelligence and its impacts on our daily lives?\"\n",
    "    },\n",
    "    RunnableConfig(metadata={\"username\": username, \"session_id\": session_id}),\n",
    ")\n",
    "display(\n",
    "    Markdown(\n",
    "        f\"\"\"\n",
    "                 **Google Output**\n",
    "                 \n",
    "                 {google_output}\n",
    "                 \n",
    "                 **Anthropic Output**\n",
    "                 \n",
    "                 {anthropic_output}\n",
    "                 \n",
    "                 **Mistral Output**\n",
    "                 \n",
    "                 {mistral_output}\n",
    "                 \"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tagging Prompts, Output Parsers, etc\n",
    "More than just LLM and Run can be tagged.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = choice([\"barweiss@cisco.com\", \"user1@cisco.com\", \"user2@cisco.com\"])\n",
    "session_id = str(uuid4())\n",
    "\n",
    "template = \"You are a very friendly and helpful Assistant. Please help the user with their queries. User: {query}\"\n",
    "prompt = ChatPromptTemplate.from_template(template=template)\n",
    "tagging_test = (\n",
    "    prompt.with_config({\"tags\": [\"friendly\", \"helpful\"]})\n",
    "    | mistral_llm.with_config({\"run_name\": \"Friendly_Mistral\"})\n",
    "    | StrOutputParser()\n",
    ")\n",
    "tagging_test = tagging_test.with_config({\"run_name\": \"Friendly_Mistral\"})\n",
    "tagging_test.invoke(\n",
    "    {\n",
    "        \"query\": \"Can you create a social media post for my blog about Artificial Intelligence and its impacts on our daily lives?\"\n",
    "    },\n",
    "    config=RunnableConfig(metadata={\"username\": username, \"session_id\": session_id}),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangSmith SDK and Tracer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tracers.context import tracing_v2_enabled\n",
    "from langsmith import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Client for interacting with the LangSmith API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "print(client.info, \"\\n\")\n",
    "print(client.read_project(project_name=\"barry-local-dev\"), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the tests results of a all runs a project as a panadas DataFrame object. Pandas needs to be installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = client.get_test_results(project_name=\"barry-local-dev\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the tracer used in a context mannager you can leverage a call back to get information back about a run without logging to LangSmith.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tracing_v2_enabled() as cb:\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        template=\"You are a helpful and harmless AI assistant. \\n\\nHuman: {query}\"\n",
    "    )\n",
    "    basic_query_a = (\n",
    "        prompt.with_config({\"tags\": [\"Tracing Example\"]})\n",
    "        | anthropic_llm.with_config(\n",
    "            {\"run_name\": \"Anthropic-Example\", \"tags\": [\"Tracing Example\"]}\n",
    "        )\n",
    "        | StrOutputParser().with_config({\"tags\": [\"Tracing Example\"]})\n",
    "    )\n",
    "    basic_query_a = basic_query_a.with_config({\"run_name\": \"Tracing Example\"})\n",
    "\n",
    "    output = basic_query_a.invoke(\n",
    "        {\n",
    "            \"query\": \"Can you create a social media post for my blog about Artificial Intelligence and its impacts on our daily lives?\"\n",
    "        }\n",
    "    )\n",
    "    print(f\"Output: {output}\\n\\n\")\n",
    "    print(f\"UUID FOR LATEST ID: {cb.latest_run.id}\")\n",
    "    print(f\"URL LINK TO RUN {cb.latest_run.id}: {cb.get_run_url()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the SDK to get the run information as a `Run` object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = client.read_run(cb.latest_run.id)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langsmith\n",
    "from langchain import smith\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Replace with the chat model you want to test\n",
    "my_llm = ChatGoogleGenerativeAI(google_api_key=google_api_key, model=\"gemini-pro\")\n",
    "\n",
    "# Define the evaluators to apply\n",
    "eval_config = smith.RunEvalConfig(\n",
    "    evaluators=[\"cot_qa\"],\n",
    "    custom_evaluators=[],\n",
    "    eval_llm=ChatOpenAI(\n",
    "        model=\"gpt-4-turbo\", temperature=0, openai_api_key=openai_api_key\n",
    "    ),\n",
    ")\n",
    "\n",
    "client = langsmith.Client()\n",
    "chain_results = client.run_on_dataset(\n",
    "    dataset_name=\"Weather API Generative Output\",\n",
    "    llm_or_chain_factory=my_llm,\n",
    "    evaluation=eval_config,\n",
    "    project_name=str(uuid4()),\n",
    "    concurrency_level=5,\n",
    "    verbose=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langsmith-py3.11.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
